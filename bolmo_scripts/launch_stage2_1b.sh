NAME=stage2_bolmo_1b
NUM_WORKERS=24 \
OLMO_ARCH=olmo2_1B_v2 \
SEQUENCE_LENGTH=4096 \
DATA_SOURCE=data_sources.txt \
LOCAL_MODEL_STYLE="hnet:xlstm" \
ADD_HASH_EMBEDDINGS=false \
ADD_EXPANDED_EMBEDDINGS=true \
LR_SCHEDULE=linear_with_warmup \
STAGE1_CKPT_PATH=/path/to/stage1/ckpt \
GLOBAL_MODEL_LEARNING_RATE=2.6e-5 \
SAVE_FOLDER=/path/to/save/folder/$NAME \
python3 src/examples/bolmo/train_stage2.py $NAME \
    train_module.optim.lr=5.2e-5 \
    data_loader.seed=1234 \
    data_loader.global_batch_size=1572864 \
    train_module.rank_microbatch_size=98304 \
    train_module.bolmo_config.losses=[ce,boundary] \
    train_module.bolmo_config.loss_weights=[1,4] \
    train_module.bolmo_config.teacher_force_boundaries=false \
    train_module.bolmo_config.do_alm_debiasing=false \
    train_module.bolmo_config.merge_boundary_loss=false \
    train_module.optim.weight_decay=0.1 \
    train_module.optim.betas=[0.9,0.95] \
    train_module.max_grad_norm=0.5 \
    model.block.attention.use_flash=true \
    model.local_encoder.n_layers=1 \
    model.local_decoder.n_layers=4 \
    model.local_decoder.hnet_smooth=false \
    model.local_decoder.hnet_modulate=false \
    model.local_encoder.boundary_predictor_lookahead=1 \
    model.local_decoder.add_in_projection=true \
    model.local_decoder.add_norm_onto_residual=false \
    model.local_decoder.add_projected_patch_residuals=false \
    model.local_encoder.block_config.feed_forward.hidden_size=2816 \
    model.local_decoder.block_config.feed_forward.hidden_size=2816 \
    model.local_encoder.d_model=2048 \
    model.local_decoder.d_model=2048 \
    trainer.callbacks.checkpointer.ephemeral_save_interval=1000 \
    trainer.callbacks.checkpointer.save_interval=30000 \
    trainer.callbacks.downstream_evaluator.eval_interval=150000 \
    trainer.max_duration.value=150000